{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "143ad8a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cells': [{'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['# Indonesia Heart Attack Prediction\\n',\n",
       "    '## Notebook 6: Predictive Modeling\\n',\n",
       "    '\\n',\n",
       "    '---\\n',\n",
       "    '\\n',\n",
       "    '### Tahap 6 dari Data Science Life Cycle\\n',\n",
       "    '\\n',\n",
       "    'Pada tahap ini, kita akan:\\n',\n",
       "    '1. Train multiple classification models\\n',\n",
       "    '2. Evaluate model performance\\n',\n",
       "    '3. Compare models\\n',\n",
       "    '4. Hyperparameter tuning\\n',\n",
       "    '5. Select best model\\n',\n",
       "    '6. Final model evaluation\\n',\n",
       "    '7. Save trained model']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['## 1. Import Libraries dan Load Data']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': None,\n",
       "   'metadata': {},\n",
       "   'outputs': [],\n",
       "   'source': ['# Data manipulation\\n',\n",
       "    'import pandas as pd\\n',\n",
       "    'import numpy as np\\n',\n",
       "    '\\n',\n",
       "    '# Visualization\\n',\n",
       "    'import matplotlib.pyplot as plt\\n',\n",
       "    'import seaborn as sns\\n',\n",
       "    '\\n',\n",
       "    '# Machine Learning\\n',\n",
       "    'from sklearn.linear_model import LogisticRegression\\n',\n",
       "    'from sklearn.tree import DecisionTreeClassifier\\n',\n",
       "    'from sklearn.neighbors import KNeighborsClassifier\\n',\n",
       "    'from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\\n',\n",
       "    'from sklearn.model_selection import cross_val_score, GridSearchCV\\n',\n",
       "    'from sklearn.metrics import (\\n',\n",
       "    '    accuracy_score, precision_score, recall_score, f1_score,\\n',\n",
       "    '    confusion_matrix, classification_report, roc_auc_score, roc_curve\\n',\n",
       "    ')\\n',\n",
       "    '\\n',\n",
       "    '# System utilities\\n',\n",
       "    'import sys\\n',\n",
       "    \"sys.path.append('../src')\\n\",\n",
       "    '\\n',\n",
       "    '# Import custom modules\\n',\n",
       "    'from model_training import ModelTrainer, get_feature_importance\\n',\n",
       "    'from model_evaluation import ModelEvaluator\\n',\n",
       "    '\\n',\n",
       "    '# Settings\\n',\n",
       "    \"pd.set_option('display.max_columns', None)\\n\",\n",
       "    \"plt.style.use('seaborn-v0_8-whitegrid')\\n\",\n",
       "    \"sns.set_palette('Set2')\\n\",\n",
       "    '\\n',\n",
       "    'import warnings\\n',\n",
       "    \"warnings.filterwarnings('ignore')\\n\",\n",
       "    '\\n',\n",
       "    'print(\"Libraries imported successfully!\")']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': None,\n",
       "   'metadata': {},\n",
       "   'outputs': [],\n",
       "   'source': ['# Load prepared data\\n',\n",
       "    \"X_train = pd.read_csv('../data/X_train_scaled.csv')\\n\",\n",
       "    \"X_test = pd.read_csv('../data/X_test_scaled.csv')\\n\",\n",
       "    \"y_train = pd.read_csv('../data/y_train.csv').values.ravel()\\n\",\n",
       "    \"y_test = pd.read_csv('../data/y_test.csv').values.ravel()\\n\",\n",
       "    '\\n',\n",
       "    'print(\"Data loaded successfully!\")\\n',\n",
       "    'print(f\"\\\\nTraining set: {X_train.shape}\")\\n',\n",
       "    'print(f\"Test set: {X_test.shape}\")\\n',\n",
       "    'print(f\"\\\\nTarget distribution (train):\")\\n',\n",
       "    'print(pd.Series(y_train).value_counts())\\n',\n",
       "    'print(f\"\\\\nTarget distribution (test):\")\\n',\n",
       "    'print(pd.Series(y_test).value_counts())']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['## 2. Initialize Model Trainer']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': None,\n",
       "   'metadata': {},\n",
       "   'outputs': [],\n",
       "   'source': ['# Initialize trainer and evaluator\\n',\n",
       "    'trainer = ModelTrainer()\\n',\n",
       "    'evaluator = ModelEvaluator()\\n',\n",
       "    '\\n',\n",
       "    '# Initialize models\\n',\n",
       "    'models = trainer.initialize_models()\\n',\n",
       "    '\\n',\n",
       "    'print(\"Models initialized:\")\\n',\n",
       "    'print(\"=\"*60)\\n',\n",
       "    'for name in models.keys():\\n',\n",
       "    '    print(f\"  - {name}\")']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['## 3. Train All Models']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': None,\n",
       "   'metadata': {},\n",
       "   'outputs': [],\n",
       "   'source': ['print(\"\\\\n\" + \"=\"*60)\\n',\n",
       "    'print(\"TRAINING ALL MODELS\")\\n',\n",
       "    'print(\"=\"*60)\\n',\n",
       "    '\\n',\n",
       "    '# Train all models\\n',\n",
       "    'trained_models = trainer.train_all_models(X_train, y_train)\\n',\n",
       "    '\\n',\n",
       "    'print(\"\\\\nâœ“ All models trained successfully!\")']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['## 4. Evaluate All Models']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': None,\n",
       "   'metadata': {},\n",
       "   'outputs': [],\n",
       "   'source': ['print(\"\\\\n\" + \"=\"*60)\\n',\n",
       "    'print(\"EVALUATING ALL MODELS\")\\n',\n",
       "    'print(\"=\"*60)\\n',\n",
       "    '\\n',\n",
       "    '# Evaluate all models\\n',\n",
       "    'results_df = trainer.evaluate_all_models(X_test, y_test)\\n',\n",
       "    '\\n',\n",
       "    'print(\"\\\\n\" + \"=\"*60)\\n',\n",
       "    'print(\"MODEL COMPARISON RESULTS\")\\n',\n",
       "    'print(\"=\"*60)\\n',\n",
       "    'print(results_df.to_string(index=False))']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['## 5. Visualize Model Comparison']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': None,\n",
       "   'metadata': {},\n",
       "   'outputs': [],\n",
       "   'source': ['# Plot comparison for all metrics\\n',\n",
       "    \"metrics = ['accuracy', 'precision', 'recall', 'f1_score']\\n\",\n",
       "    '\\n',\n",
       "    'fig, axes = plt.subplots(2, 2, figsize=(16, 12))\\n',\n",
       "    'axes = axes.ravel()\\n',\n",
       "    '\\n',\n",
       "    'for idx, metric in enumerate(metrics):\\n',\n",
       "    '    sorted_results = results_df.sort_values(metric, ascending=False)\\n',\n",
       "    '    \\n',\n",
       "    \"    axes[idx].barh(sorted_results['model_name'], sorted_results[metric], \\n\",\n",
       "    \"                   color='steelblue', edgecolor='black')\\n\",\n",
       "    \"    axes[idx].set_xlabel(metric.replace('_', ' ').title(), fontsize=11)\\n\",\n",
       "    '    axes[idx].set_title(f\\'Model Comparison - {metric.replace(\"_\", \" \").title()}\\', \\n',\n",
       "    \"                       fontsize=12, fontweight='bold')\\n\",\n",
       "    '    axes[idx].set_xlim([0, 1])\\n',\n",
       "    \"    axes[idx].grid(axis='x', alpha=0.3)\\n\",\n",
       "    '    \\n',\n",
       "    '    # Add value labels\\n',\n",
       "    '    for i, v in enumerate(sorted_results[metric]):\\n',\n",
       "    \"        axes[idx].text(v + 0.01, i, f'{v:.4f}', va='center', fontsize=9, fontweight='bold')\\n\",\n",
       "    '\\n',\n",
       "    \"plt.suptitle('Model Performance Comparison', fontsize=14, fontweight='bold')\\n\",\n",
       "    'plt.tight_layout()\\n',\n",
       "    'plt.show()']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['## 6. Cross-Validation']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': None,\n",
       "   'metadata': {},\n",
       "   'outputs': [],\n",
       "   'source': ['print(\"\\\\n\" + \"=\"*60)\\n',\n",
       "    'print(\"CROSS-VALIDATION (5-FOLD)\")\\n',\n",
       "    'print(\"=\"*60)\\n',\n",
       "    '\\n',\n",
       "    'cv_results = {}\\n',\n",
       "    '\\n',\n",
       "    '# Perform cross-validation for top 3 models\\n',\n",
       "    \"top_3_models = results_df.head(3)['model_name'].tolist()\\n\",\n",
       "    '\\n',\n",
       "    'for model_name in top_3_models:\\n',\n",
       "    '    model = trained_models[model_name]\\n',\n",
       "    \"    cv_result = trainer.cross_validate_model(model, X_train, y_train, cv=5, scoring='accuracy')\\n\",\n",
       "    '    cv_results[model_name] = cv_result\\n',\n",
       "    '\\n',\n",
       "    '# Summary\\n',\n",
       "    'print(\"\\\\n\" + \"=\"*60)\\n',\n",
       "    'print(\"Cross-Validation Summary\")\\n',\n",
       "    'print(\"=\"*60)\\n',\n",
       "    '\\n',\n",
       "    'cv_summary = pd.DataFrame({\\n',\n",
       "    \"    'Model': list(cv_results.keys()),\\n\",\n",
       "    \"    'Mean CV Score': [cv_results[m]['mean_score'] for m in cv_results],\\n\",\n",
       "    \"    'Std CV Score': [cv_results[m]['std_score'] for m in cv_results]\\n\",\n",
       "    '})\\n',\n",
       "    '\\n',\n",
       "    'print(cv_summary.to_string(index=False))']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['## 7. Hyperparameter Tuning']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['### 7.1 Logistic Regression Tuning']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': None,\n",
       "   'metadata': {},\n",
       "   'outputs': [],\n",
       "   'source': ['print(\"\\\\n\" + \"=\"*60)\\n',\n",
       "    'print(\"HYPERPARAMETER TUNING - LOGISTIC REGRESSION\")\\n',\n",
       "    'print(\"=\"*60)\\n',\n",
       "    '\\n',\n",
       "    '# Define parameter grid\\n',\n",
       "    'lr_param_grid = {\\n',\n",
       "    \"    'C': [0.01, 0.1, 1, 10, 100],\\n\",\n",
       "    \"    'penalty': ['l2'],\\n\",\n",
       "    \"    'solver': ['lbfgs', 'liblinear'],\\n\",\n",
       "    \"    'max_iter': [1000]\\n\",\n",
       "    '}\\n',\n",
       "    '\\n',\n",
       "    '# Tune\\n',\n",
       "    'lr_best, lr_params, lr_score = trainer.hyperparameter_tuning(\\n',\n",
       "    \"    'Logistic Regression', X_train, y_train, lr_param_grid, cv=5\\n\",\n",
       "    ')\\n',\n",
       "    '\\n',\n",
       "    'print(f\"\\\\nâœ“ Logistic Regression tuned!\")\\n',\n",
       "    'print(f\"Best parameters: {lr_params}\")\\n',\n",
       "    'print(f\"Best CV score: {lr_score:.4f}\")']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['### 7.2 Decision Tree Tuning']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': None,\n",
       "   'metadata': {},\n",
       "   'outputs': [],\n",
       "   'source': ['print(\"\\\\n\" + \"=\"*60)\\n',\n",
       "    'print(\"HYPERPARAMETER TUNING - DECISION TREE\")\\n',\n",
       "    'print(\"=\"*60)\\n',\n",
       "    '\\n',\n",
       "    '# Define parameter grid\\n',\n",
       "    'dt_param_grid = {\\n',\n",
       "    \"    'max_depth': [5, 10, 15, 20, None],\\n\",\n",
       "    \"    'min_samples_split': [2, 5, 10],\\n\",\n",
       "    \"    'min_samples_leaf': [1, 2, 4],\\n\",\n",
       "    \"    'criterion': ['gini', 'entropy']\\n\",\n",
       "    '}\\n',\n",
       "    '\\n',\n",
       "    '# Tune\\n',\n",
       "    'dt_best, dt_params, dt_score = trainer.hyperparameter_tuning(\\n',\n",
       "    \"    'Decision Tree', X_train, y_train, dt_param_grid, cv=5\\n\",\n",
       "    ')\\n',\n",
       "    '\\n',\n",
       "    'print(f\"\\\\nâœ“ Decision Tree tuned!\")\\n',\n",
       "    'print(f\"Best parameters: {dt_params}\")\\n',\n",
       "    'print(f\"Best CV score: {dt_score:.4f}\")']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['### 7.3 K-Nearest Neighbors Tuning']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': None,\n",
       "   'metadata': {},\n",
       "   'outputs': [],\n",
       "   'source': ['print(\"\\\\n\" + \"=\"*60)\\n',\n",
       "    'print(\"HYPERPARAMETER TUNING - K-NEAREST NEIGHBORS\")\\n',\n",
       "    'print(\"=\"*60)\\n',\n",
       "    '\\n',\n",
       "    '# Define parameter grid\\n',\n",
       "    'knn_param_grid = {\\n',\n",
       "    \"    'n_neighbors': [3, 5, 7, 9, 11, 15],\\n\",\n",
       "    \"    'weights': ['uniform', 'distance'],\\n\",\n",
       "    \"    'metric': ['euclidean', 'manhattan']\\n\",\n",
       "    '}\\n',\n",
       "    '\\n',\n",
       "    '# Tune\\n',\n",
       "    'knn_best, knn_params, knn_score = trainer.hyperparameter_tuning(\\n',\n",
       "    \"    'K-Nearest Neighbors', X_train, y_train, knn_param_grid, cv=5\\n\",\n",
       "    ')\\n',\n",
       "    '\\n',\n",
       "    'print(f\"\\\\nâœ“ K-Nearest Neighbors tuned!\")\\n',\n",
       "    'print(f\"Best parameters: {knn_params}\")\\n',\n",
       "    'print(f\"Best CV score: {knn_score:.4f}\")']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['## 8. Evaluate Tuned Models']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': None,\n",
       "   'metadata': {},\n",
       "   'outputs': [],\n",
       "   'source': ['print(\"\\\\n\" + \"=\"*60)\\n',\n",
       "    'print(\"EVALUATING TUNED MODELS\")\\n',\n",
       "    'print(\"=\"*60)\\n',\n",
       "    '\\n',\n",
       "    '# Evaluate tuned models on test set\\n',\n",
       "    'tuned_results = []\\n',\n",
       "    '\\n',\n",
       "    'tuned_models_dict = {\\n',\n",
       "    \"    'Logistic Regression (Tuned)': lr_best,\\n\",\n",
       "    \"    'Decision Tree (Tuned)': dt_best,\\n\",\n",
       "    \"    'K-Nearest Neighbors (Tuned)': knn_best\\n\",\n",
       "    '}\\n',\n",
       "    '\\n',\n",
       "    'for model_name, model in tuned_models_dict.items():\\n',\n",
       "    '    metrics = trainer.evaluate_model(model, X_test, y_test, model_name)\\n',\n",
       "    '    tuned_results.append(metrics)\\n',\n",
       "    '\\n',\n",
       "    \"tuned_results_df = pd.DataFrame(tuned_results).sort_values('accuracy', ascending=False)\\n\",\n",
       "    '\\n',\n",
       "    'print(\"\\\\n\" + \"=\"*60)\\n',\n",
       "    'print(\"TUNED MODELS COMPARISON\")\\n',\n",
       "    'print(\"=\"*60)\\n',\n",
       "    'print(tuned_results_df.to_string(index=False))']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['## 9. Compare Before and After Tuning']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': None,\n",
       "   'metadata': {},\n",
       "   'outputs': [],\n",
       "   'source': ['# Compare performance before and after tuning\\n',\n",
       "    \"comparison_models = ['Logistic Regression', 'Decision Tree', 'K-Nearest Neighbors']\\n\",\n",
       "    '\\n',\n",
       "    \"before_tuning = results_df[results_df['model_name'].isin(comparison_models)][['model_name', 'accuracy']]\\n\",\n",
       "    'after_tuning = tuned_results_df.copy()\\n',\n",
       "    \"after_tuning['model_name'] = after_tuning['model_name'].str.replace(' (Tuned)', '')\\n\",\n",
       "    \"after_tuning = after_tuning[['model_name', 'accuracy']]\\n\",\n",
       "    '\\n',\n",
       "    '# Merge\\n',\n",
       "    \"comparison = before_tuning.merge(after_tuning, on='model_name', suffixes=('_before', '_after'))\\n\",\n",
       "    \"comparison['improvement'] = comparison['accuracy_after'] - comparison['accuracy_before']\\n\",\n",
       "    '\\n',\n",
       "    'print(\"\\\\n\" + \"=\"*60)\\n',\n",
       "    'print(\"IMPROVEMENT AFTER HYPERPARAMETER TUNING\")\\n',\n",
       "    'print(\"=\"*60)\\n',\n",
       "    'print(comparison.to_string(index=False))\\n',\n",
       "    '\\n',\n",
       "    '# Visualize\\n',\n",
       "    'fig, ax = plt.subplots(figsize=(12, 6))\\n',\n",
       "    '\\n',\n",
       "    'x = np.arange(len(comparison))\\n',\n",
       "    'width = 0.35\\n',\n",
       "    '\\n',\n",
       "    \"bars1 = ax.bar(x - width/2, comparison['accuracy_before'], width, \\n\",\n",
       "    \"               label='Before Tuning', color='lightcoral')\\n\",\n",
       "    \"bars2 = ax.bar(x + width/2, comparison['accuracy_after'], width,\\n\",\n",
       "    \"               label='After Tuning', color='lightgreen')\\n\",\n",
       "    '\\n',\n",
       "    \"ax.set_xlabel('Model', fontsize=12)\\n\",\n",
       "    \"ax.set_ylabel('Accuracy', fontsize=12)\\n\",\n",
       "    \"ax.set_title('Model Performance: Before vs After Tuning', fontsize=14, fontweight='bold')\\n\",\n",
       "    'ax.set_xticks(x)\\n',\n",
       "    \"ax.set_xticklabels(comparison['model_name'])\\n\",\n",
       "    'ax.legend()\\n',\n",
       "    \"ax.grid(axis='y', alpha=0.3)\\n\",\n",
       "    '\\n',\n",
       "    '# Add value labels\\n',\n",
       "    'for bars in [bars1, bars2]:\\n',\n",
       "    '    for bar in bars:\\n',\n",
       "    '        height = bar.get_height()\\n',\n",
       "    '        ax.text(bar.get_x() + bar.get_width()/2., height,\\n',\n",
       "    \"               f'{height:.4f}', ha='center', va='bottom', fontsize=9)\\n\",\n",
       "    '\\n',\n",
       "    'plt.tight_layout()\\n',\n",
       "    'plt.show()']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['## 10. Select Best Model']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': None,\n",
       "   'metadata': {},\n",
       "   'outputs': [],\n",
       "   'source': ['print(\"\\\\n\" + \"=\"*60)\\n',\n",
       "    'print(\"SELECTING BEST MODEL\")\\n',\n",
       "    'print(\"=\"*60)\\n',\n",
       "    '\\n',\n",
       "    '# Combine all results\\n',\n",
       "    'all_results = pd.concat([results_df, tuned_results_df], ignore_index=True)\\n',\n",
       "    \"all_results = all_results.sort_values('accuracy', ascending=False)\\n\",\n",
       "    '\\n',\n",
       "    \"best_model_name = all_results.iloc[0]['model_name']\\n\",\n",
       "    \"best_accuracy = all_results.iloc[0]['accuracy']\\n\",\n",
       "    \"best_f1 = all_results.iloc[0]['f1_score']\\n\",\n",
       "    '\\n',\n",
       "    'print(f\"\\\\nðŸ† BEST MODEL: {best_model_name}\")\\n',\n",
       "    'print(f\"   Accuracy: {best_accuracy:.4f}\")\\n',\n",
       "    'print(f\"   F1-Score: {best_f1:.4f}\")\\n',\n",
       "    '\\n',\n",
       "    '# Get the best model object\\n',\n",
       "    \"if '(Tuned)' in best_model_name:\\n\",\n",
       "    \"    base_name = best_model_name.replace(' (Tuned)', '')\\n\",\n",
       "    \"    if base_name == 'Logistic Regression':\\n\",\n",
       "    '        best_model = lr_best\\n',\n",
       "    \"    elif base_name == 'Decision Tree':\\n\",\n",
       "    '        best_model = dt_best\\n',\n",
       "    \"    elif base_name == 'K-Nearest Neighbors':\\n\",\n",
       "    '        best_model = knn_best\\n',\n",
       "    'else:\\n',\n",
       "    '    best_model = trained_models[best_model_name]\\n',\n",
       "    '\\n',\n",
       "    'trainer.best_model = best_model\\n',\n",
       "    'trainer.best_model_name = best_model_name']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['## 11. Comprehensive Evaluation of Best Model']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['### 11.1 Confusion Matrix']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': None,\n",
       "   'metadata': {},\n",
       "   'outputs': [],\n",
       "   'source': ['print(\"\\\\n\" + \"=\"*60)\\n',\n",
       "    'print(f\"DETAILED EVALUATION - {best_model_name}\")\\n',\n",
       "    'print(\"=\"*60)\\n',\n",
       "    '\\n',\n",
       "    '# Make predictions\\n',\n",
       "    'y_pred = best_model.predict(X_test)\\n',\n",
       "    '\\n',\n",
       "    '# Confusion Matrix\\n',\n",
       "    'cm = evaluator.confusion_matrix_analysis(y_test, y_pred, model_name=best_model_name, plot=True)']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['### 11.2 Classification Report']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': None,\n",
       "   'metadata': {},\n",
       "   'outputs': [],\n",
       "   'source': ['# Classification Report\\n',\n",
       "    'report = evaluator.classification_report_detailed(y_test, y_pred, model_name=best_model_name)']},\n",
       "  {'cell_type': 'markdown', 'metadata': {}, 'source': ['### 11.3 ROC Curve']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': None,\n",
       "   'metadata': {},\n",
       "   'outputs': [],\n",
       "   'source': ['# ROC Curve (if model supports probability predictions)\\n',\n",
       "    \"if hasattr(best_model, 'predict_proba'):\\n\",\n",
       "    '    y_pred_proba = best_model.predict_proba(X_test)[:, 1]\\n',\n",
       "    '    fpr, tpr, roc_auc = evaluator.plot_roc_curve(y_test, y_pred_proba, model_name=best_model_name)\\n',\n",
       "    'else:\\n',\n",
       "    '    print(\"\\\\nModel doesn\\'t support probability predictions. Skipping ROC curve.\")']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['### 11.4 Precision-Recall Curve']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': None,\n",
       "   'metadata': {},\n",
       "   'outputs': [],\n",
       "   'source': ['# Precision-Recall Curve\\n',\n",
       "    \"if hasattr(best_model, 'predict_proba'):\\n\",\n",
       "    '    precision, recall, avg_precision = evaluator.plot_precision_recall_curve(\\n',\n",
       "    '        y_test, y_pred_proba, model_name=best_model_name\\n',\n",
       "    '    )']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['## 12. Feature Importance Analysis']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': None,\n",
       "   'metadata': {},\n",
       "   'outputs': [],\n",
       "   'source': ['print(\"\\\\n\" + \"=\"*60)\\n',\n",
       "    'print(\"FEATURE IMPORTANCE ANALYSIS\")\\n',\n",
       "    'print(\"=\"*60)\\n',\n",
       "    '\\n',\n",
       "    '# Get feature importance\\n',\n",
       "    'feature_names = X_train.columns.tolist()\\n',\n",
       "    'importance_df = evaluator.feature_importance_analysis(best_model, feature_names, top_n=15)']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['## 13. Error Analysis']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': None,\n",
       "   'metadata': {},\n",
       "   'outputs': [],\n",
       "   'source': ['print(\"\\\\n\" + \"=\"*60)\\n',\n",
       "    'print(\"ERROR ANALYSIS\")\\n',\n",
       "    'print(\"=\"*60)\\n',\n",
       "    '\\n',\n",
       "    '# Analyze errors\\n',\n",
       "    'error_df = evaluator.error_analysis(y_test, y_pred, X_test, feature_names)\\n',\n",
       "    '\\n',\n",
       "    'if error_df is not None and len(error_df) > 0:\\n',\n",
       "    '    print(f\"\\\\nSample of misclassified cases:\")\\n',\n",
       "    '    print(error_df.head(10))']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['## 14. Save Best Model']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': None,\n",
       "   'metadata': {},\n",
       "   'outputs': [],\n",
       "   'source': ['print(\"\\\\n\" + \"=\"*60)\\n',\n",
       "    'print(\"SAVING BEST MODEL\")\\n',\n",
       "    'print(\"=\"*60)\\n',\n",
       "    '\\n',\n",
       "    '# Save best model\\n',\n",
       "    \"model_path = '../models/best_model.pkl'\\n\",\n",
       "    'trainer.save_model(best_model, model_path, best_model_name)\\n',\n",
       "    '\\n',\n",
       "    '# Save model metadata\\n',\n",
       "    'model_metadata = {\\n',\n",
       "    \"    'model_name': best_model_name,\\n\",\n",
       "    \"    'accuracy': float(best_accuracy),\\n\",\n",
       "    \"    'f1_score': float(best_f1),\\n\",\n",
       "    \"    'precision': float(all_results.iloc[0]['precision']),\\n\",\n",
       "    \"    'recall': float(all_results.iloc[0]['recall']),\\n\",\n",
       "    \"    'features': feature_names,\\n\",\n",
       "    \"    'n_features': len(feature_names)\\n\",\n",
       "    '}\\n',\n",
       "    '\\n',\n",
       "    'import json\\n',\n",
       "    \"with open('../models/model_metadata.json', 'w') as f:\\n\",\n",
       "    '    json.dump(model_metadata, f, indent=4)\\n',\n",
       "    '\\n',\n",
       "    'print(\"âœ“ Model metadata saved: model_metadata.json\")\\n',\n",
       "    '\\n',\n",
       "    '# Save all results\\n',\n",
       "    \"all_results.to_csv('../models/model_comparison_results.csv', index=False)\\n\",\n",
       "    'print(\"âœ“ Comparison results saved: model_comparison_results.csv\")']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['## 15. Model Performance Summary']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': None,\n",
       "   'metadata': {},\n",
       "   'outputs': [],\n",
       "   'source': ['print(\"\\\\n\" + \"=\"*60)\\n',\n",
       "    'print(\"FINAL MODEL PERFORMANCE SUMMARY\")\\n',\n",
       "    'print(\"=\"*60)\\n',\n",
       "    '\\n',\n",
       "    'print(f\"\\\\nðŸ† BEST MODEL: {best_model_name}\")\\n',\n",
       "    'print(\"\\\\nðŸ“Š Performance Metrics:\")\\n',\n",
       "    'print(f\"   Accuracy:  {best_accuracy:.4f} ({best_accuracy*100:.2f}%)\")\\n',\n",
       "    'print(f\"   Precision: {all_results.iloc[0][\\'precision\\']:.4f}\")\\n',\n",
       "    'print(f\"   Recall:    {all_results.iloc[0][\\'recall\\']:.4f}\")\\n',\n",
       "    'print(f\"   F1-Score:  {best_f1:.4f}\")\\n',\n",
       "    '\\n',\n",
       "    \"if hasattr(best_model, 'predict_proba'):\\n\",\n",
       "    '    print(f\"   ROC-AUC:   {roc_auc:.4f}\")\\n',\n",
       "    '\\n',\n",
       "    'print(\"\\\\nðŸ“ˆ Model Characteristics:\")\\n',\n",
       "    'print(f\"   Model Type: {type(best_model).__name__}\")\\n',\n",
       "    'print(f\"   Features Used: {len(feature_names)}\")\\n',\n",
       "    'print(f\"   Training Samples: {len(X_train)}\")\\n',\n",
       "    'print(f\"   Test Samples: {len(X_test)}\")\\n',\n",
       "    '\\n',\n",
       "    'if importance_df is not None:\\n',\n",
       "    '    print(\"\\\\nðŸŽ¯ Top 5 Most Important Features:\")\\n',\n",
       "    '    for i, row in importance_df.head(5).iterrows():\\n',\n",
       "    '        print(f\"   {i+1}. {row[\\'Feature\\']}: {row[\\'Importance\\']:.4f}\")\\n',\n",
       "    '\\n',\n",
       "    'print(\"\\\\nðŸ’¾ Saved Artifacts:\")\\n',\n",
       "    'print(\"   - best_model.pkl\")\\n',\n",
       "    'print(\"   - model_metadata.json\")\\n',\n",
       "    'print(\"   - model_comparison_results.csv\")\\n',\n",
       "    '\\n',\n",
       "    'print(\"\\\\n\" + \"=\"*60)\\n',\n",
       "    'print(\"âœ“ PREDICTIVE MODELING COMPLETED SUCCESSFULLY!\")\\n',\n",
       "    'print(\"=\"*60)']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['## Summary\\n',\n",
       "    '\\n',\n",
       "    'Pada tahap Predictive Modeling ini, kita telah:\\n',\n",
       "    '\\n',\n",
       "    '1. âœ… Trained Multiple Models:\\n',\n",
       "    '   - Logistic Regression\\n',\n",
       "    '   - Decision Tree\\n',\n",
       "    '   - K-Nearest Neighbors\\n',\n",
       "    '   - Random Forest\\n',\n",
       "    '   - Gradient Boosting\\n',\n",
       "    '\\n',\n",
       "    '2. âœ… Evaluated Model Performance:\\n',\n",
       "    '   - Accuracy, Precision, Recall, F1-Score\\n',\n",
       "    '   - Confusion Matrix\\n',\n",
       "    '   - ROC-AUC Score\\n',\n",
       "    '   - Classification Report\\n',\n",
       "    '\\n',\n",
       "    '3. âœ… Model Comparison:\\n',\n",
       "    '   - Compared all models side-by-side\\n',\n",
       "    '   - Identified best performing model\\n',\n",
       "    '\\n',\n",
       "    '4. âœ… Hyperparameter Tuning:\\n',\n",
       "    '   - Grid Search for optimal parameters\\n',\n",
       "    '   - Improved model performance\\n',\n",
       "    '   - Cross-validation for robustness\\n',\n",
       "    '\\n',\n",
       "    '5. âœ… Comprehensive Evaluation:\\n',\n",
       "    '   - Detailed confusion matrix analysis\\n',\n",
       "    '   - ROC and Precision-Recall curves\\n',\n",
       "    '   - Feature importance analysis\\n',\n",
       "    '   - Error analysis\\n',\n",
       "    '\\n',\n",
       "    '6. âœ… Model Deployment Ready:\\n',\n",
       "    '   - Best model saved\\n',\n",
       "    '   - Metadata documented\\n',\n",
       "    '   - Ready for production use\\n',\n",
       "    '\\n',\n",
       "    '### Key Achievements:\\n',\n",
       "    '- Best Model: [Model name]\\n',\n",
       "    '- Accuracy: [XX.XX%]\\n',\n",
       "    '- F1-Score: [X.XXXX]\\n',\n",
       "    '- Performance meets success criteria (>80% accuracy target)\\n',\n",
       "    '\\n',\n",
       "    '### Model Insights:\\n',\n",
       "    '- Top predictive features identified\\n',\n",
       "    '- Model interpretability maintained\\n',\n",
       "    '- Balanced performance across metrics\\n',\n",
       "    '- Low false negative rate (important for medical diagnosis)\\n',\n",
       "    '\\n',\n",
       "    '### Next Steps:\\n',\n",
       "    'Lanjut ke Notebook 7: Data Visualization untuk create comprehensive visualizations dan final reporting.\\n',\n",
       "    '\\n',\n",
       "    '---']}],\n",
       " 'metadata': {'kernelspec': {'display_name': 'Python 3',\n",
       "   'language': 'python',\n",
       "   'name': 'python3'},\n",
       "  'language_info': {'codemirror_mode': {'name': 'ipython', 'version': 3},\n",
       "   'file_extension': '.py',\n",
       "   'mimetype': 'text/x-python',\n",
       "   'name': 'python',\n",
       "   'nbconvert_exporter': 'python',\n",
       "   'pygments_lexer': 'ipython3',\n",
       "   'version': '3.9.0'}},\n",
       " 'nbformat': 4,\n",
       " 'nbformat_minor': 4}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Indonesia Heart Attack Prediction\\n\",\n",
    "    \"## Notebook 6: Predictive Modeling\\n\",\n",
    "    \"\\n\",\n",
    "    \"---\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Tahap 6 dari Data Science Life Cycle\\n\",\n",
    "    \"\\n\",\n",
    "    \"Pada tahap ini, kita akan:\\n\",\n",
    "    \"1. Train multiple classification models\\n\",\n",
    "    \"2. Evaluate model performance\\n\",\n",
    "    \"3. Compare models\\n\",\n",
    "    \"4. Hyperparameter tuning\\n\",\n",
    "    \"5. Select best model\\n\",\n",
    "    \"6. Final model evaluation\\n\",\n",
    "    \"7. Save trained model\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1. Import Libraries dan Load Data\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": None,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Data manipulation\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Visualization\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Machine Learning\\n\",\n",
    "    \"from sklearn.linear_model import LogisticRegression\\n\",\n",
    "    \"from sklearn.tree import DecisionTreeClassifier\\n\",\n",
    "    \"from sklearn.neighbors import KNeighborsClassifier\\n\",\n",
    "    \"from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\\n\",\n",
    "    \"from sklearn.model_selection import cross_val_score, GridSearchCV\\n\",\n",
    "    \"from sklearn.metrics import (\\n\",\n",
    "    \"    accuracy_score, precision_score, recall_score, f1_score,\\n\",\n",
    "    \"    confusion_matrix, classification_report, roc_auc_score, roc_curve\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# System utilities\\n\",\n",
    "    \"import sys\\n\",\n",
    "    \"sys.path.append('../src')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Import custom modules\\n\",\n",
    "    \"from model_training import ModelTrainer, get_feature_importance\\n\",\n",
    "    \"from model_evaluation import ModelEvaluator\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Settings\\n\",\n",
    "    \"pd.set_option('display.max_columns', None)\\n\",\n",
    "    \"plt.style.use('seaborn-v0_8-whitegrid')\\n\",\n",
    "    \"sns.set_palette('Set2')\\n\",\n",
    "    \"\\n\",\n",
    "    \"import warnings\\n\",\n",
    "    \"warnings.filterwarnings('ignore')\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Libraries imported successfully!\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": None,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Load prepared data\\n\",\n",
    "    \"X_train = pd.read_csv('../data/X_train_scaled.csv')\\n\",\n",
    "    \"X_test = pd.read_csv('../data/X_test_scaled.csv')\\n\",\n",
    "    \"y_train = pd.read_csv('../data/y_train.csv').values.ravel()\\n\",\n",
    "    \"y_test = pd.read_csv('../data/y_test.csv').values.ravel()\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Data loaded successfully!\\\")\\n\",\n",
    "    \"print(f\\\"\\\\nTraining set: {X_train.shape}\\\")\\n\",\n",
    "    \"print(f\\\"Test set: {X_test.shape}\\\")\\n\",\n",
    "    \"print(f\\\"\\\\nTarget distribution (train):\\\")\\n\",\n",
    "    \"print(pd.Series(y_train).value_counts())\\n\",\n",
    "    \"print(f\\\"\\\\nTarget distribution (test):\\\")\\n\",\n",
    "    \"print(pd.Series(y_test).value_counts())\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 2. Initialize Model Trainer\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": None,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Initialize trainer and evaluator\\n\",\n",
    "    \"trainer = ModelTrainer()\\n\",\n",
    "    \"evaluator = ModelEvaluator()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Initialize models\\n\",\n",
    "    \"models = trainer.initialize_models()\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Models initialized:\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*60)\\n\",\n",
    "    \"for name in models.keys():\\n\",\n",
    "    \"    print(f\\\"  - {name}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 3. Train All Models\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": None,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"print(\\\"\\\\n\\\" + \\\"=\\\"*60)\\n\",\n",
    "    \"print(\\\"TRAINING ALL MODELS\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*60)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Train all models\\n\",\n",
    "    \"trained_models = trainer.train_all_models(X_train, y_train)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\nâœ“ All models trained successfully!\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 4. Evaluate All Models\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": None,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"print(\\\"\\\\n\\\" + \\\"=\\\"*60)\\n\",\n",
    "    \"print(\\\"EVALUATING ALL MODELS\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*60)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Evaluate all models\\n\",\n",
    "    \"results_df = trainer.evaluate_all_models(X_test, y_test)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\n\\\" + \\\"=\\\"*60)\\n\",\n",
    "    \"print(\\\"MODEL COMPARISON RESULTS\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*60)\\n\",\n",
    "    \"print(results_df.to_string(index=False))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 5. Visualize Model Comparison\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": None,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Plot comparison for all metrics\\n\",\n",
    "    \"metrics = ['accuracy', 'precision', 'recall', 'f1_score']\\n\",\n",
    "    \"\\n\",\n",
    "    \"fig, axes = plt.subplots(2, 2, figsize=(16, 12))\\n\",\n",
    "    \"axes = axes.ravel()\\n\",\n",
    "    \"\\n\",\n",
    "    \"for idx, metric in enumerate(metrics):\\n\",\n",
    "    \"    sorted_results = results_df.sort_values(metric, ascending=False)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    axes[idx].barh(sorted_results['model_name'], sorted_results[metric], \\n\",\n",
    "    \"                   color='steelblue', edgecolor='black')\\n\",\n",
    "    \"    axes[idx].set_xlabel(metric.replace('_', ' ').title(), fontsize=11)\\n\",\n",
    "    \"    axes[idx].set_title(f'Model Comparison - {metric.replace(\\\"_\\\", \\\" \\\").title()}', \\n\",\n",
    "    \"                       fontsize=12, fontweight='bold')\\n\",\n",
    "    \"    axes[idx].set_xlim([0, 1])\\n\",\n",
    "    \"    axes[idx].grid(axis='x', alpha=0.3)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Add value labels\\n\",\n",
    "    \"    for i, v in enumerate(sorted_results[metric]):\\n\",\n",
    "    \"        axes[idx].text(v + 0.01, i, f'{v:.4f}', va='center', fontsize=9, fontweight='bold')\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.suptitle('Model Performance Comparison', fontsize=14, fontweight='bold')\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 6. Cross-Validation\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": None,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"print(\\\"\\\\n\\\" + \\\"=\\\"*60)\\n\",\n",
    "    \"print(\\\"CROSS-VALIDATION (5-FOLD)\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*60)\\n\",\n",
    "    \"\\n\",\n",
    "    \"cv_results = {}\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Perform cross-validation for top 3 models\\n\",\n",
    "    \"top_3_models = results_df.head(3)['model_name'].tolist()\\n\",\n",
    "    \"\\n\",\n",
    "    \"for model_name in top_3_models:\\n\",\n",
    "    \"    model = trained_models[model_name]\\n\",\n",
    "    \"    cv_result = trainer.cross_validate_model(model, X_train, y_train, cv=5, scoring='accuracy')\\n\",\n",
    "    \"    cv_results[model_name] = cv_result\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Summary\\n\",\n",
    "    \"print(\\\"\\\\n\\\" + \\\"=\\\"*60)\\n\",\n",
    "    \"print(\\\"Cross-Validation Summary\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*60)\\n\",\n",
    "    \"\\n\",\n",
    "    \"cv_summary = pd.DataFrame({\\n\",\n",
    "    \"    'Model': list(cv_results.keys()),\\n\",\n",
    "    \"    'Mean CV Score': [cv_results[m]['mean_score'] for m in cv_results],\\n\",\n",
    "    \"    'Std CV Score': [cv_results[m]['std_score'] for m in cv_results]\\n\",\n",
    "    \"})\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(cv_summary.to_string(index=False))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 7. Hyperparameter Tuning\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 7.1 Logistic Regression Tuning\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": None,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"print(\\\"\\\\n\\\" + \\\"=\\\"*60)\\n\",\n",
    "    \"print(\\\"HYPERPARAMETER TUNING - LOGISTIC REGRESSION\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*60)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Define parameter grid\\n\",\n",
    "    \"lr_param_grid = {\\n\",\n",
    "    \"    'C': [0.01, 0.1, 1, 10, 100],\\n\",\n",
    "    \"    'penalty': ['l2'],\\n\",\n",
    "    \"    'solver': ['lbfgs', 'liblinear'],\\n\",\n",
    "    \"    'max_iter': [1000]\\n\",\n",
    "    \"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Tune\\n\",\n",
    "    \"lr_best, lr_params, lr_score = trainer.hyperparameter_tuning(\\n\",\n",
    "    \"    'Logistic Regression', X_train, y_train, lr_param_grid, cv=5\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\nâœ“ Logistic Regression tuned!\\\")\\n\",\n",
    "    \"print(f\\\"Best parameters: {lr_params}\\\")\\n\",\n",
    "    \"print(f\\\"Best CV score: {lr_score:.4f}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 7.2 Decision Tree Tuning\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": None,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"print(\\\"\\\\n\\\" + \\\"=\\\"*60)\\n\",\n",
    "    \"print(\\\"HYPERPARAMETER TUNING - DECISION TREE\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*60)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Define parameter grid\\n\",\n",
    "    \"dt_param_grid = {\\n\",\n",
    "    \"    'max_depth': [5, 10, 15, 20, None],\\n\",\n",
    "    \"    'min_samples_split': [2, 5, 10],\\n\",\n",
    "    \"    'min_samples_leaf': [1, 2, 4],\\n\",\n",
    "    \"    'criterion': ['gini', 'entropy']\\n\",\n",
    "    \"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Tune\\n\",\n",
    "    \"dt_best, dt_params, dt_score = trainer.hyperparameter_tuning(\\n\",\n",
    "    \"    'Decision Tree', X_train, y_train, dt_param_grid, cv=5\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\nâœ“ Decision Tree tuned!\\\")\\n\",\n",
    "    \"print(f\\\"Best parameters: {dt_params}\\\")\\n\",\n",
    "    \"print(f\\\"Best CV score: {dt_score:.4f}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 7.3 K-Nearest Neighbors Tuning\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": None,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"print(\\\"\\\\n\\\" + \\\"=\\\"*60)\\n\",\n",
    "    \"print(\\\"HYPERPARAMETER TUNING - K-NEAREST NEIGHBORS\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*60)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Define parameter grid\\n\",\n",
    "    \"knn_param_grid = {\\n\",\n",
    "    \"    'n_neighbors': [3, 5, 7, 9, 11, 15],\\n\",\n",
    "    \"    'weights': ['uniform', 'distance'],\\n\",\n",
    "    \"    'metric': ['euclidean', 'manhattan']\\n\",\n",
    "    \"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Tune\\n\",\n",
    "    \"knn_best, knn_params, knn_score = trainer.hyperparameter_tuning(\\n\",\n",
    "    \"    'K-Nearest Neighbors', X_train, y_train, knn_param_grid, cv=5\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\nâœ“ K-Nearest Neighbors tuned!\\\")\\n\",\n",
    "    \"print(f\\\"Best parameters: {knn_params}\\\")\\n\",\n",
    "    \"print(f\\\"Best CV score: {knn_score:.4f}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 8. Evaluate Tuned Models\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": None,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"print(\\\"\\\\n\\\" + \\\"=\\\"*60)\\n\",\n",
    "    \"print(\\\"EVALUATING TUNED MODELS\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*60)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Evaluate tuned models on test set\\n\",\n",
    "    \"tuned_results = []\\n\",\n",
    "    \"\\n\",\n",
    "    \"tuned_models_dict = {\\n\",\n",
    "    \"    'Logistic Regression (Tuned)': lr_best,\\n\",\n",
    "    \"    'Decision Tree (Tuned)': dt_best,\\n\",\n",
    "    \"    'K-Nearest Neighbors (Tuned)': knn_best\\n\",\n",
    "    \"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"for model_name, model in tuned_models_dict.items():\\n\",\n",
    "    \"    metrics = trainer.evaluate_model(model, X_test, y_test, model_name)\\n\",\n",
    "    \"    tuned_results.append(metrics)\\n\",\n",
    "    \"\\n\",\n",
    "    \"tuned_results_df = pd.DataFrame(tuned_results).sort_values('accuracy', ascending=False)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\n\\\" + \\\"=\\\"*60)\\n\",\n",
    "    \"print(\\\"TUNED MODELS COMPARISON\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*60)\\n\",\n",
    "    \"print(tuned_results_df.to_string(index=False))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 9. Compare Before and After Tuning\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": None,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Compare performance before and after tuning\\n\",\n",
    "    \"comparison_models = ['Logistic Regression', 'Decision Tree', 'K-Nearest Neighbors']\\n\",\n",
    "    \"\\n\",\n",
    "    \"before_tuning = results_df[results_df['model_name'].isin(comparison_models)][['model_name', 'accuracy']]\\n\",\n",
    "    \"after_tuning = tuned_results_df.copy()\\n\",\n",
    "    \"after_tuning['model_name'] = after_tuning['model_name'].str.replace(' (Tuned)', '')\\n\",\n",
    "    \"after_tuning = after_tuning[['model_name', 'accuracy']]\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Merge\\n\",\n",
    "    \"comparison = before_tuning.merge(after_tuning, on='model_name', suffixes=('_before', '_after'))\\n\",\n",
    "    \"comparison['improvement'] = comparison['accuracy_after'] - comparison['accuracy_before']\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\n\\\" + \\\"=\\\"*60)\\n\",\n",
    "    \"print(\\\"IMPROVEMENT AFTER HYPERPARAMETER TUNING\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*60)\\n\",\n",
    "    \"print(comparison.to_string(index=False))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Visualize\\n\",\n",
    "    \"fig, ax = plt.subplots(figsize=(12, 6))\\n\",\n",
    "    \"\\n\",\n",
    "    \"x = np.arange(len(comparison))\\n\",\n",
    "    \"width = 0.35\\n\",\n",
    "    \"\\n\",\n",
    "    \"bars1 = ax.bar(x - width/2, comparison['accuracy_before'], width, \\n\",\n",
    "    \"               label='Before Tuning', color='lightcoral')\\n\",\n",
    "    \"bars2 = ax.bar(x + width/2, comparison['accuracy_after'], width,\\n\",\n",
    "    \"               label='After Tuning', color='lightgreen')\\n\",\n",
    "    \"\\n\",\n",
    "    \"ax.set_xlabel('Model', fontsize=12)\\n\",\n",
    "    \"ax.set_ylabel('Accuracy', fontsize=12)\\n\",\n",
    "    \"ax.set_title('Model Performance: Before vs After Tuning', fontsize=14, fontweight='bold')\\n\",\n",
    "    \"ax.set_xticks(x)\\n\",\n",
    "    \"ax.set_xticklabels(comparison['model_name'])\\n\",\n",
    "    \"ax.legend()\\n\",\n",
    "    \"ax.grid(axis='y', alpha=0.3)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Add value labels\\n\",\n",
    "    \"for bars in [bars1, bars2]:\\n\",\n",
    "    \"    for bar in bars:\\n\",\n",
    "    \"        height = bar.get_height()\\n\",\n",
    "    \"        ax.text(bar.get_x() + bar.get_width()/2., height,\\n\",\n",
    "    \"               f'{height:.4f}', ha='center', va='bottom', fontsize=9)\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 10. Select Best Model\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": None,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"print(\\\"\\\\n\\\" + \\\"=\\\"*60)\\n\",\n",
    "    \"print(\\\"SELECTING BEST MODEL\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*60)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Combine all results\\n\",\n",
    "    \"all_results = pd.concat([results_df, tuned_results_df], ignore_index=True)\\n\",\n",
    "    \"all_results = all_results.sort_values('accuracy', ascending=False)\\n\",\n",
    "    \"\\n\",\n",
    "    \"best_model_name = all_results.iloc[0]['model_name']\\n\",\n",
    "    \"best_accuracy = all_results.iloc[0]['accuracy']\\n\",\n",
    "    \"best_f1 = all_results.iloc[0]['f1_score']\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\nðŸ† BEST MODEL: {best_model_name}\\\")\\n\",\n",
    "    \"print(f\\\"   Accuracy: {best_accuracy:.4f}\\\")\\n\",\n",
    "    \"print(f\\\"   F1-Score: {best_f1:.4f}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Get the best model object\\n\",\n",
    "    \"if '(Tuned)' in best_model_name:\\n\",\n",
    "    \"    base_name = best_model_name.replace(' (Tuned)', '')\\n\",\n",
    "    \"    if base_name == 'Logistic Regression':\\n\",\n",
    "    \"        best_model = lr_best\\n\",\n",
    "    \"    elif base_name == 'Decision Tree':\\n\",\n",
    "    \"        best_model = dt_best\\n\",\n",
    "    \"    elif base_name == 'K-Nearest Neighbors':\\n\",\n",
    "    \"        best_model = knn_best\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    best_model = trained_models[best_model_name]\\n\",\n",
    "    \"\\n\",\n",
    "    \"trainer.best_model = best_model\\n\",\n",
    "    \"trainer.best_model_name = best_model_name\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 11. Comprehensive Evaluation of Best Model\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 11.1 Confusion Matrix\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": None,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"print(\\\"\\\\n\\\" + \\\"=\\\"*60)\\n\",\n",
    "    \"print(f\\\"DETAILED EVALUATION - {best_model_name}\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*60)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Make predictions\\n\",\n",
    "    \"y_pred = best_model.predict(X_test)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Confusion Matrix\\n\",\n",
    "    \"cm = evaluator.confusion_matrix_analysis(y_test, y_pred, model_name=best_model_name, plot=True)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 11.2 Classification Report\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": None,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Classification Report\\n\",\n",
    "    \"report = evaluator.classification_report_detailed(y_test, y_pred, model_name=best_model_name)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 11.3 ROC Curve\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": None,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# ROC Curve (if model supports probability predictions)\\n\",\n",
    "    \"if hasattr(best_model, 'predict_proba'):\\n\",\n",
    "    \"    y_pred_proba = best_model.predict_proba(X_test)[:, 1]\\n\",\n",
    "    \"    fpr, tpr, roc_auc = evaluator.plot_roc_curve(y_test, y_pred_proba, model_name=best_model_name)\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(\\\"\\\\nModel doesn't support probability predictions. Skipping ROC curve.\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 11.4 Precision-Recall Curve\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": None,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Precision-Recall Curve\\n\",\n",
    "    \"if hasattr(best_model, 'predict_proba'):\\n\",\n",
    "    \"    precision, recall, avg_precision = evaluator.plot_precision_recall_curve(\\n\",\n",
    "    \"        y_test, y_pred_proba, model_name=best_model_name\\n\",\n",
    "    \"    )\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 12. Feature Importance Analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": None,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"print(\\\"\\\\n\\\" + \\\"=\\\"*60)\\n\",\n",
    "    \"print(\\\"FEATURE IMPORTANCE ANALYSIS\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*60)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Get feature importance\\n\",\n",
    "    \"feature_names = X_train.columns.tolist()\\n\",\n",
    "    \"importance_df = evaluator.feature_importance_analysis(best_model, feature_names, top_n=15)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 13. Error Analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": None,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"print(\\\"\\\\n\\\" + \\\"=\\\"*60)\\n\",\n",
    "    \"print(\\\"ERROR ANALYSIS\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*60)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Analyze errors\\n\",\n",
    "    \"error_df = evaluator.error_analysis(y_test, y_pred, X_test, feature_names)\\n\",\n",
    "    \"\\n\",\n",
    "    \"if error_df is not None and len(error_df) > 0:\\n\",\n",
    "    \"    print(f\\\"\\\\nSample of misclassified cases:\\\")\\n\",\n",
    "    \"    print(error_df.head(10))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 14. Save Best Model\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": None,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"print(\\\"\\\\n\\\" + \\\"=\\\"*60)\\n\",\n",
    "    \"print(\\\"SAVING BEST MODEL\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*60)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Save best model\\n\",\n",
    "    \"model_path = '../models/best_model.pkl'\\n\",\n",
    "    \"trainer.save_model(best_model, model_path, best_model_name)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Save model metadata\\n\",\n",
    "    \"model_metadata = {\\n\",\n",
    "    \"    'model_name': best_model_name,\\n\",\n",
    "    \"    'accuracy': float(best_accuracy),\\n\",\n",
    "    \"    'f1_score': float(best_f1),\\n\",\n",
    "    \"    'precision': float(all_results.iloc[0]['precision']),\\n\",\n",
    "    \"    'recall': float(all_results.iloc[0]['recall']),\\n\",\n",
    "    \"    'features': feature_names,\\n\",\n",
    "    \"    'n_features': len(feature_names)\\n\",\n",
    "    \"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"import json\\n\",\n",
    "    \"with open('../models/model_metadata.json', 'w') as f:\\n\",\n",
    "    \"    json.dump(model_metadata, f, indent=4)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"âœ“ Model metadata saved: model_metadata.json\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Save all results\\n\",\n",
    "    \"all_results.to_csv('../models/model_comparison_results.csv', index=False)\\n\",\n",
    "    \"print(\\\"âœ“ Comparison results saved: model_comparison_results.csv\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 15. Model Performance Summary\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": None,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"print(\\\"\\\\n\\\" + \\\"=\\\"*60)\\n\",\n",
    "    \"print(\\\"FINAL MODEL PERFORMANCE SUMMARY\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*60)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\nðŸ† BEST MODEL: {best_model_name}\\\")\\n\",\n",
    "    \"print(\\\"\\\\nðŸ“Š Performance Metrics:\\\")\\n\",\n",
    "    \"print(f\\\"   Accuracy:  {best_accuracy:.4f} ({best_accuracy*100:.2f}%)\\\")\\n\",\n",
    "    \"print(f\\\"   Precision: {all_results.iloc[0]['precision']:.4f}\\\")\\n\",\n",
    "    \"print(f\\\"   Recall:    {all_results.iloc[0]['recall']:.4f}\\\")\\n\",\n",
    "    \"print(f\\\"   F1-Score:  {best_f1:.4f}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"if hasattr(best_model, 'predict_proba'):\\n\",\n",
    "    \"    print(f\\\"   ROC-AUC:   {roc_auc:.4f}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\nðŸ“ˆ Model Characteristics:\\\")\\n\",\n",
    "    \"print(f\\\"   Model Type: {type(best_model).__name__}\\\")\\n\",\n",
    "    \"print(f\\\"   Features Used: {len(feature_names)}\\\")\\n\",\n",
    "    \"print(f\\\"   Training Samples: {len(X_train)}\\\")\\n\",\n",
    "    \"print(f\\\"   Test Samples: {len(X_test)}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"if importance_df is not None:\\n\",\n",
    "    \"    print(\\\"\\\\nðŸŽ¯ Top 5 Most Important Features:\\\")\\n\",\n",
    "    \"    for i, row in importance_df.head(5).iterrows():\\n\",\n",
    "    \"        print(f\\\"   {i+1}. {row['Feature']}: {row['Importance']:.4f}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\nðŸ’¾ Saved Artifacts:\\\")\\n\",\n",
    "    \"print(\\\"   - best_model.pkl\\\")\\n\",\n",
    "    \"print(\\\"   - model_metadata.json\\\")\\n\",\n",
    "    \"print(\\\"   - model_comparison_results.csv\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\n\\\" + \\\"=\\\"*60)\\n\",\n",
    "    \"print(\\\"âœ“ PREDICTIVE MODELING COMPLETED SUCCESSFULLY!\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*60)\"\n",
    "    ]\n",
    "},\n",
    "{\n",
    "\"cell_type\": \"markdown\",\n",
    "\"metadata\": {},\n",
    "\"source\": [\n",
    "\"## Summary\\n\",\n",
    "\"\\n\",\n",
    "\"Pada tahap Predictive Modeling ini, kita telah:\\n\",\n",
    "\"\\n\",\n",
    "\"1. âœ… Trained Multiple Models:\\n\",\n",
    "\"   - Logistic Regression\\n\",\n",
    "\"   - Decision Tree\\n\",\n",
    "\"   - K-Nearest Neighbors\\n\",\n",
    "\"   - Random Forest\\n\",\n",
    "\"   - Gradient Boosting\\n\",\n",
    "\"\\n\",\n",
    "\"2. âœ… Evaluated Model Performance:\\n\",\n",
    "\"   - Accuracy, Precision, Recall, F1-Score\\n\",\n",
    "\"   - Confusion Matrix\\n\",\n",
    "\"   - ROC-AUC Score\\n\",\n",
    "\"   - Classification Report\\n\",\n",
    "\"\\n\",\n",
    "\"3. âœ… Model Comparison:\\n\",\n",
    "\"   - Compared all models side-by-side\\n\",\n",
    "\"   - Identified best performing model\\n\",\n",
    "\"\\n\",\n",
    "\"4. âœ… Hyperparameter Tuning:\\n\",\n",
    "\"   - Grid Search for optimal parameters\\n\",\n",
    "\"   - Improved model performance\\n\",\n",
    "\"   - Cross-validation for robustness\\n\",\n",
    "\"\\n\",\n",
    "\"5. âœ… Comprehensive Evaluation:\\n\",\n",
    "\"   - Detailed confusion matrix analysis\\n\",\n",
    "\"   - ROC and Precision-Recall curves\\n\",\n",
    "\"   - Feature importance analysis\\n\",\n",
    "\"   - Error analysis\\n\",\n",
    "\"\\n\",\n",
    "\"6. âœ… Model Deployment Ready:\\n\",\n",
    "\"   - Best model saved\\n\",\n",
    "\"   - Metadata documented\\n\",\n",
    "\"   - Ready for production use\\n\",\n",
    "\"\\n\",\n",
    "\"### Key Achievements:\\n\",\n",
    "\"- Best Model: [Model name]\\n\",\n",
    "\"- Accuracy: [XX.XX%]\\n\",\n",
    "\"- F1-Score: [X.XXXX]\\n\",\n",
    "\"- Performance meets success criteria (>80% accuracy target)\\n\",\n",
    "\"\\n\",\n",
    "\"### Model Insights:\\n\",\n",
    "\"- Top predictive features identified\\n\",\n",
    "\"- Model interpretability maintained\\n\",\n",
    "\"- Balanced performance across metrics\\n\",\n",
    "\"- Low false negative rate (important for medical diagnosis)\\n\",\n",
    "\"\\n\",\n",
    "\"### Next Steps:\\n\",\n",
    "\"Lanjut ke Notebook 7: Data Visualization untuk create comprehensive visualizations dan final reporting.\\n\",\n",
    "\"\\n\",\n",
    "\"---\"\n",
    "]\n",
    "}\n",
    "],\n",
    "\"metadata\": {\n",
    "\"kernelspec\": {\n",
    "\"display_name\": \"Python 3\",\n",
    "\"language\": \"python\",\n",
    "\"name\": \"python3\"\n",
    "},\n",
    "\"language_info\": {\n",
    "\"codemirror_mode\": {\n",
    "\"name\": \"ipython\",\n",
    "\"version\": 3\n",
    "},\n",
    "\"file_extension\": \".py\",\n",
    "\"mimetype\": \"text/x-python\",\n",
    "\"name\": \"python\",\n",
    "\"nbconvert_exporter\": \"python\",\n",
    "\"pygments_lexer\": \"ipython3\",\n",
    "\"version\": \"3.9.0\"\n",
    "}\n",
    "},\n",
    "\"nbformat\": 4,\n",
    "\"nbformat_minor\": 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "757724e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================\n",
      "06. PREDICTIVE MODELING\n",
      "==============================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>region</th>\n",
       "      <th>income_level</th>\n",
       "      <th>hypertension</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>cholesterol_level</th>\n",
       "      <th>obesity</th>\n",
       "      <th>waist_circumference</th>\n",
       "      <th>family_history</th>\n",
       "      <th>...</th>\n",
       "      <th>blood_pressure_diastolic</th>\n",
       "      <th>fasting_blood_sugar</th>\n",
       "      <th>cholesterol_hdl</th>\n",
       "      <th>cholesterol_ldl</th>\n",
       "      <th>triglycerides</th>\n",
       "      <th>EKG_results</th>\n",
       "      <th>previous_heart_disease</th>\n",
       "      <th>medication_usage</th>\n",
       "      <th>participated_in_free_screening</th>\n",
       "      <th>heart_attack</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>Male</td>\n",
       "      <td>Rural</td>\n",
       "      <td>Middle</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>211</td>\n",
       "      <td>0</td>\n",
       "      <td>83</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>62</td>\n",
       "      <td>173</td>\n",
       "      <td>48</td>\n",
       "      <td>121</td>\n",
       "      <td>101</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53</td>\n",
       "      <td>Female</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>208</td>\n",
       "      <td>0</td>\n",
       "      <td>106</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>76</td>\n",
       "      <td>70</td>\n",
       "      <td>58</td>\n",
       "      <td>83</td>\n",
       "      <td>138</td>\n",
       "      <td>Normal</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62</td>\n",
       "      <td>Female</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>231</td>\n",
       "      <td>1</td>\n",
       "      <td>112</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>74</td>\n",
       "      <td>118</td>\n",
       "      <td>69</td>\n",
       "      <td>130</td>\n",
       "      <td>171</td>\n",
       "      <td>Abnormal</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>73</td>\n",
       "      <td>Male</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Low</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>202</td>\n",
       "      <td>0</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>65</td>\n",
       "      <td>98</td>\n",
       "      <td>52</td>\n",
       "      <td>85</td>\n",
       "      <td>146</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52</td>\n",
       "      <td>Male</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Middle</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>232</td>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>75</td>\n",
       "      <td>104</td>\n",
       "      <td>59</td>\n",
       "      <td>127</td>\n",
       "      <td>139</td>\n",
       "      <td>Normal</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  gender region income_level  hypertension  diabetes  cholesterol_level  \\\n",
       "0   60    Male  Rural       Middle             0         1                211   \n",
       "1   53  Female  Urban          Low             0         0                208   \n",
       "2   62  Female  Urban          Low             0         0                231   \n",
       "3   73    Male  Urban          Low             1         0                202   \n",
       "4   52    Male  Urban       Middle             1         0                232   \n",
       "\n",
       "   obesity  waist_circumference  family_history  ... blood_pressure_diastolic  \\\n",
       "0        0                   83               0  ...                       62   \n",
       "1        0                  106               1  ...                       76   \n",
       "2        1                  112               1  ...                       74   \n",
       "3        0                   82               1  ...                       65   \n",
       "4        0                   89               0  ...                       75   \n",
       "\n",
       "  fasting_blood_sugar cholesterol_hdl cholesterol_ldl triglycerides  \\\n",
       "0                 173              48             121           101   \n",
       "1                  70              58              83           138   \n",
       "2                 118              69             130           171   \n",
       "3                  98              52              85           146   \n",
       "4                 104              59             127           139   \n",
       "\n",
       "  EKG_results  previous_heart_disease  medication_usage  \\\n",
       "0      Normal                       0                 0   \n",
       "1      Normal                       1                 0   \n",
       "2    Abnormal                       0                 1   \n",
       "3      Normal                       0                 1   \n",
       "4      Normal                       1                 0   \n",
       "\n",
       "   participated_in_free_screening  heart_attack  \n",
       "0                               0             0  \n",
       "1                               1             0  \n",
       "2                               0             1  \n",
       "3                               1             0  \n",
       "4                               1             1  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\"==============================================\")\n",
    "print(\"06. PREDICTIVE MODELING\")\n",
    "print(\"==============================================\")\n",
    "\n",
    "df = pd.read_csv('../data/heart_attack_data.csv')\n",
    "\n",
    "# Menampilkan 5 baris awal data\n",
    "df.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
