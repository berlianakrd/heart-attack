{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143ad8a8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Indonesia Heart Attack Prediction\\n\",\n",
    "    \"## Notebook 6: Predictive Modeling\\n\",\n",
    "    \"\\n\",\n",
    "    \"---\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Tahap 6 dari Data Science Life Cycle\\n\",\n",
    "    \"\\n\",\n",
    "    \"Pada tahap ini, kita akan:\\n\",\n",
    "    \"1. Train multiple classification models\\n\",\n",
    "    \"2. Evaluate model performance\\n\",\n",
    "    \"3. Compare models\\n\",\n",
    "    \"4. Hyperparameter tuning\\n\",\n",
    "    \"5. Select best model\\n\",\n",
    "    \"6. Final model evaluation\\n\",\n",
    "    \"7. Save trained model\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1. Import Libraries dan Load Data\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Data manipulation\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Visualization\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Machine Learning\\n\",\n",
    "    \"from sklearn.linear_model import LogisticRegression\\n\",\n",
    "    \"from sklearn.tree import DecisionTreeClassifier\\n\",\n",
    "    \"from sklearn.neighbors import KNeighborsClassifier\\n\",\n",
    "    \"from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\\n\",\n",
    "    \"from sklearn.model_selection import cross_val_score, GridSearchCV\\n\",\n",
    "    \"from sklearn.metrics import (\\n\",\n",
    "    \"    accuracy_score, precision_score, recall_score, f1_score,\\n\",\n",
    "    \"    confusion_matrix, classification_report, roc_auc_score, roc_curve\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# System utilities\\n\",\n",
    "    \"import sys\\n\",\n",
    "    \"sys.path.append('../src')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Import custom modules\\n\",\n",
    "    \"from model_training import ModelTrainer, get_feature_importance\\n\",\n",
    "    \"from model_evaluation import ModelEvaluator\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Settings\\n\",\n",
    "    \"pd.set_option('display.max_columns', None)\\n\",\n",
    "    \"plt.style.use('seaborn-v0_8-whitegrid')\\n\",\n",
    "    \"sns.set_palette('Set2')\\n\",\n",
    "    \"\\n\",\n",
    "    \"import warnings\\n\",\n",
    "    \"warnings.filterwarnings('ignore')\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Libraries imported successfully!\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Load prepared data\\n\",\n",
    "    \"X_train = pd.read_csv('../data/X_train_scaled.csv')\\n\",\n",
    "    \"X_test = pd.read_csv('../data/X_test_scaled.csv')\\n\",\n",
    "    \"y_train = pd.read_csv('../data/y_train.csv').values.ravel()\\n\",\n",
    "    \"y_test = pd.read_csv('../data/y_test.csv').values.ravel()\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Data loaded successfully!\\\")\\n\",\n",
    "    \"print(f\\\"\\\\nTraining set: {X_train.shape}\\\")\\n\",\n",
    "    \"print(f\\\"Test set: {X_test.shape}\\\")\\n\",\n",
    "    \"print(f\\\"\\\\nTarget distribution (train):\\\")\\n\",\n",
    "    \"print(pd.Series(y_train).value_counts())\\n\",\n",
    "    \"print(f\\\"\\\\nTarget distribution (test):\\\")\\n\",\n",
    "    \"print(pd.Series(y_test).value_counts())\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 2. Initialize Model Trainer\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Initialize trainer and evaluator\\n\",\n",
    "    \"trainer = ModelTrainer()\\n\",\n",
    "    \"evaluator = ModelEvaluator()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Initialize models\\n\",\n",
    "    \"models = trainer.initialize_models()\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Models initialized:\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*60)\\n\",\n",
    "    \"for name in models.keys():\\n\",\n",
    "    \"    print(f\\\"  - {name}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 3. Train All Models\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"print(\\\"\\\\n\\\" + \\\"=\\\"*60)\\n\",\n",
    "    \"print(\\\"TRAINING ALL MODELS\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*60)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Train all models\\n\",\n",
    "    \"trained_models = trainer.train_all_models(X_train, y_train)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\nâœ“ All models trained successfully!\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 4. Evaluate All Models\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"print(\\\"\\\\n\\\" + \\\"=\\\"*60)\\n\",\n",
    "    \"print(\\\"EVALUATING ALL MODELS\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*60)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Evaluate all models\\n\",\n",
    "    \"results_df = trainer.evaluate_all_models(X_test, y_test)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\n\\\" + \\\"=\\\"*60)\\n\",\n",
    "    \"print(\\\"MODEL COMPARISON RESULTS\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*60)\\n\",\n",
    "    \"print(results_df.to_string(index=False))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 5. Visualize Model Comparison\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Plot comparison for all metrics\\n\",\n",
    "    \"metrics = ['accuracy', 'precision', 'recall', 'f1_score']\\n\",\n",
    "    \"\\n\",\n",
    "    \"fig, axes = plt.subplots(2, 2, figsize=(16, 12))\\n\",\n",
    "    \"axes = axes.ravel()\\n\",\n",
    "    \"\\n\",\n",
    "    \"for idx, metric in enumerate(metrics):\\n\",\n",
    "    \"    sorted_results = results_df.sort_values(metric, ascending=False)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    axes[idx].barh(sorted_results['model_name'], sorted_results[metric], \\n\",\n",
    "    \"                   color='steelblue', edgecolor='black')\\n\",\n",
    "    \"    axes[idx].set_xlabel(metric.replace('_', ' ').title(), fontsize=11)\\n\",\n",
    "    \"    axes[idx].set_title(f'Model Comparison - {metric.replace(\\\"_\\\", \\\" \\\").title()}', \\n\",\n",
    "    \"                       fontsize=12, fontweight='bold')\\n\",\n",
    "    \"    axes[idx].set_xlim([0, 1])\\n\",\n",
    "    \"    axes[idx].grid(axis='x', alpha=0.3)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Add value labels\\n\",\n",
    "    \"    for i, v in enumerate(sorted_results[metric]):\\n\",\n",
    "    \"        axes[idx].text(v + 0.01, i, f'{v:.4f}', va='center', fontsize=9, fontweight='bold')\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.suptitle('Model Performance Comparison', fontsize=14, fontweight='bold')\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 6. Cross-Validation\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"print(\\\"\\\\n\\\" + \\\"=\\\"*60)\\n\",\n",
    "    \"print(\\\"CROSS-VALIDATION (5-FOLD)\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*60)\\n\",\n",
    "    \"\\n\",\n",
    "    \"cv_results = {}\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Perform cross-validation for top 3 models\\n\",\n",
    "    \"top_3_models = results_df.head(3)['model_name'].tolist()\\n\",\n",
    "    \"\\n\",\n",
    "    \"for model_name in top_3_models:\\n\",\n",
    "    \"    model = trained_models[model_name]\\n\",\n",
    "    \"    cv_result = trainer.cross_validate_model(model, X_train, y_train, cv=5, scoring='accuracy')\\n\",\n",
    "    \"    cv_results[model_name] = cv_result\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Summary\\n\",\n",
    "    \"print(\\\"\\\\n\\\" + \\\"=\\\"*60)\\n\",\n",
    "    \"print(\\\"Cross-Validation Summary\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*60)\\n\",\n",
    "    \"\\n\",\n",
    "    \"cv_summary = pd.DataFrame({\\n\",\n",
    "    \"    'Model': list(cv_results.keys()),\\n\",\n",
    "    \"    'Mean CV Score': [cv_results[m]['mean_score'] for m in cv_results],\\n\",\n",
    "    \"    'Std CV Score': [cv_results[m]['std_score'] for m in cv_results]\\n\",\n",
    "    \"})\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(cv_summary.to_string(index=False))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 7. Hyperparameter Tuning\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 7.1 Logistic Regression Tuning\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"print(\\\"\\\\n\\\" + \\\"=\\\"*60)\\n\",\n",
    "    \"print(\\\"HYPERPARAMETER TUNING - LOGISTIC REGRESSION\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*60)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Define parameter grid\\n\",\n",
    "    \"lr_param_grid = {\\n\",\n",
    "    \"    'C': [0.01, 0.1, 1, 10, 100],\\n\",\n",
    "    \"    'penalty': ['l2'],\\n\",\n",
    "    \"    'solver': ['lbfgs', 'liblinear'],\\n\",\n",
    "    \"    'max_iter': [1000]\\n\",\n",
    "    \"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Tune\\n\",\n",
    "    \"lr_best, lr_params, lr_score = trainer.hyperparameter_tuning(\\n\",\n",
    "    \"    'Logistic Regression', X_train, y_train, lr_param_grid, cv=5\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\nâœ“ Logistic Regression tuned!\\\")\\n\",\n",
    "    \"print(f\\\"Best parameters: {lr_params}\\\")\\n\",\n",
    "    \"print(f\\\"Best CV score: {lr_score:.4f}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 7.2 Decision Tree Tuning\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"print(\\\"\\\\n\\\" + \\\"=\\\"*60)\\n\",\n",
    "    \"print(\\\"HYPERPARAMETER TUNING - DECISION TREE\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*60)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Define parameter grid\\n\",\n",
    "    \"dt_param_grid = {\\n\",\n",
    "    \"    'max_depth': [5, 10, 15, 20, None],\\n\",\n",
    "    \"    'min_samples_split': [2, 5, 10],\\n\",\n",
    "    \"    'min_samples_leaf': [1, 2, 4],\\n\",\n",
    "    \"    'criterion': ['gini', 'entropy']\\n\",\n",
    "    \"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Tune\\n\",\n",
    "    \"dt_best, dt_params, dt_score = trainer.hyperparameter_tuning(\\n\",\n",
    "    \"    'Decision Tree', X_train, y_train, dt_param_grid, cv=5\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\nâœ“ Decision Tree tuned!\\\")\\n\",\n",
    "    \"print(f\\\"Best parameters: {dt_params}\\\")\\n\",\n",
    "    \"print(f\\\"Best CV score: {dt_score:.4f}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 7.3 K-Nearest Neighbors Tuning\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"print(\\\"\\\\n\\\" + \\\"=\\\"*60)\\n\",\n",
    "    \"print(\\\"HYPERPARAMETER TUNING - K-NEAREST NEIGHBORS\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*60)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Define parameter grid\\n\",\n",
    "    \"knn_param_grid = {\\n\",\n",
    "    \"    'n_neighbors': [3, 5, 7, 9, 11, 15],\\n\",\n",
    "    \"    'weights': ['uniform', 'distance'],\\n\",\n",
    "    \"    'metric': ['euclidean', 'manhattan']\\n\",\n",
    "    \"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Tune\\n\",\n",
    "    \"knn_best, knn_params, knn_score = trainer.hyperparameter_tuning(\\n\",\n",
    "    \"    'K-Nearest Neighbors', X_train, y_train, knn_param_grid, cv=5\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\nâœ“ K-Nearest Neighbors tuned!\\\")\\n\",\n",
    "    \"print(f\\\"Best parameters: {knn_params}\\\")\\n\",\n",
    "    \"print(f\\\"Best CV score: {knn_score:.4f}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 8. Evaluate Tuned Models\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"print(\\\"\\\\n\\\" + \\\"=\\\"*60)\\n\",\n",
    "    \"print(\\\"EVALUATING TUNED MODELS\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*60)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Evaluate tuned models on test set\\n\",\n",
    "    \"tuned_results = []\\n\",\n",
    "    \"\\n\",\n",
    "    \"tuned_models_dict = {\\n\",\n",
    "    \"    'Logistic Regression (Tuned)': lr_best,\\n\",\n",
    "    \"    'Decision Tree (Tuned)': dt_best,\\n\",\n",
    "    \"    'K-Nearest Neighbors (Tuned)': knn_best\\n\",\n",
    "    \"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"for model_name, model in tuned_models_dict.items():\\n\",\n",
    "    \"    metrics = trainer.evaluate_model(model, X_test, y_test, model_name)\\n\",\n",
    "    \"    tuned_results.append(metrics)\\n\",\n",
    "    \"\\n\",\n",
    "    \"tuned_results_df = pd.DataFrame(tuned_results).sort_values('accuracy', ascending=False)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\n\\\" + \\\"=\\\"*60)\\n\",\n",
    "    \"print(\\\"TUNED MODELS COMPARISON\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*60)\\n\",\n",
    "    \"print(tuned_results_df.to_string(index=False))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 9. Compare Before and After Tuning\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Compare performance before and after tuning\\n\",\n",
    "    \"comparison_models = ['Logistic Regression', 'Decision Tree', 'K-Nearest Neighbors']\\n\",\n",
    "    \"\\n\",\n",
    "    \"before_tuning = results_df[results_df['model_name'].isin(comparison_models)][['model_name', 'accuracy']]\\n\",\n",
    "    \"after_tuning = tuned_results_df.copy()\\n\",\n",
    "    \"after_tuning['model_name'] = after_tuning['model_name'].str.replace(' (Tuned)', '')\\n\",\n",
    "    \"after_tuning = after_tuning[['model_name', 'accuracy']]\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Merge\\n\",\n",
    "    \"comparison = before_tuning.merge(after_tuning, on='model_name', suffixes=('_before', '_after'))\\n\",\n",
    "    \"comparison['improvement'] = comparison['accuracy_after'] - comparison['accuracy_before']\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\n\\\" + \\\"=\\\"*60)\\n\",\n",
    "    \"print(\\\"IMPROVEMENT AFTER HYPERPARAMETER TUNING\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*60)\\n\",\n",
    "    \"print(comparison.to_string(index=False))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Visualize\\n\",\n",
    "    \"fig, ax = plt.subplots(figsize=(12, 6))\\n\",\n",
    "    \"\\n\",\n",
    "    \"x = np.arange(len(comparison))\\n\",\n",
    "    \"width = 0.35\\n\",\n",
    "    \"\\n\",\n",
    "    \"bars1 = ax.bar(x - width/2, comparison['accuracy_before'], width, \\n\",\n",
    "    \"               label='Before Tuning', color='lightcoral')\\n\",\n",
    "    \"bars2 = ax.bar(x + width/2, comparison['accuracy_after'], width,\\n\",\n",
    "    \"               label='After Tuning', color='lightgreen')\\n\",\n",
    "    \"\\n\",\n",
    "    \"ax.set_xlabel('Model', fontsize=12)\\n\",\n",
    "    \"ax.set_ylabel('Accuracy', fontsize=12)\\n\",\n",
    "    \"ax.set_title('Model Performance: Before vs After Tuning', fontsize=14, fontweight='bold')\\n\",\n",
    "    \"ax.set_xticks(x)\\n\",\n",
    "    \"ax.set_xticklabels(comparison['model_name'])\\n\",\n",
    "    \"ax.legend()\\n\",\n",
    "    \"ax.grid(axis='y', alpha=0.3)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Add value labels\\n\",\n",
    "    \"for bars in [bars1, bars2]:\\n\",\n",
    "    \"    for bar in bars:\\n\",\n",
    "    \"        height = bar.get_height()\\n\",\n",
    "    \"        ax.text(bar.get_x() + bar.get_width()/2., height,\\n\",\n",
    "    \"               f'{height:.4f}', ha='center', va='bottom', fontsize=9)\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 10. Select Best Model\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"print(\\\"\\\\n\\\" + \\\"=\\\"*60)\\n\",\n",
    "    \"print(\\\"SELECTING BEST MODEL\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*60)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Combine all results\\n\",\n",
    "    \"all_results = pd.concat([results_df, tuned_results_df], ignore_index=True)\\n\",\n",
    "    \"all_results = all_results.sort_values('accuracy', ascending=False)\\n\",\n",
    "    \"\\n\",\n",
    "    \"best_model_name = all_results.iloc[0]['model_name']\\n\",\n",
    "    \"best_accuracy = all_results.iloc[0]['accuracy']\\n\",\n",
    "    \"best_f1 = all_results.iloc[0]['f1_score']\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\nðŸ† BEST MODEL: {best_model_name}\\\")\\n\",\n",
    "    \"print(f\\\"   Accuracy: {best_accuracy:.4f}\\\")\\n\",\n",
    "    \"print(f\\\"   F1-Score: {best_f1:.4f}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Get the best model object\\n\",\n",
    "    \"if '(Tuned)' in best_model_name:\\n\",\n",
    "    \"    base_name = best_model_name.replace(' (Tuned)', '')\\n\",\n",
    "    \"    if base_name == 'Logistic Regression':\\n\",\n",
    "    \"        best_model = lr_best\\n\",\n",
    "    \"    elif base_name == 'Decision Tree':\\n\",\n",
    "    \"        best_model = dt_best\\n\",\n",
    "    \"    elif base_name == 'K-Nearest Neighbors':\\n\",\n",
    "    \"        best_model = knn_best\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    best_model = trained_models[best_model_name]\\n\",\n",
    "    \"\\n\",\n",
    "    \"trainer.best_model = best_model\\n\",\n",
    "    \"trainer.best_model_name = best_model_name\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 11. Comprehensive Evaluation of Best Model\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 11.1 Confusion Matrix\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"print(\\\"\\\\n\\\" + \\\"=\\\"*60)\\n\",\n",
    "    \"print(f\\\"DETAILED EVALUATION - {best_model_name}\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*60)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Make predictions\\n\",\n",
    "    \"y_pred = best_model.predict(X_test)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Confusion Matrix\\n\",\n",
    "    \"cm = evaluator.confusion_matrix_analysis(y_test, y_pred, model_name=best_model_name, plot=True)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 11.2 Classification Report\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Classification Report\\n\",\n",
    "    \"report = evaluator.classification_report_detailed(y_test, y_pred, model_name=best_model_name)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 11.3 ROC Curve\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# ROC Curve (if model supports probability predictions)\\n\",\n",
    "    \"if hasattr(best_model, 'predict_proba'):\\n\",\n",
    "    \"    y_pred_proba = best_model.predict_proba(X_test)[:, 1]\\n\",\n",
    "    \"    fpr, tpr, roc_auc = evaluator.plot_roc_curve(y_test, y_pred_proba, model_name=best_model_name)\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(\\\"\\\\nModel doesn't support probability predictions. Skipping ROC curve.\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 11.4 Precision-Recall Curve\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Precision-Recall Curve\\n\",\n",
    "    \"if hasattr(best_model, 'predict_proba'):\\n\",\n",
    "    \"    precision, recall, avg_precision = evaluator.plot_precision_recall_curve(\\n\",\n",
    "    \"        y_test, y_pred_proba, model_name=best_model_name\\n\",\n",
    "    \"    )\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 12. Feature Importance Analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"print(\\\"\\\\n\\\" + \\\"=\\\"*60)\\n\",\n",
    "    \"print(\\\"FEATURE IMPORTANCE ANALYSIS\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*60)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Get feature importance\\n\",\n",
    "    \"feature_names = X_train.columns.tolist()\\n\",\n",
    "    \"importance_df = evaluator.feature_importance_analysis(best_model, feature_names, top_n=15)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 13. Error Analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"print(\\\"\\\\n\\\" + \\\"=\\\"*60)\\n\",\n",
    "    \"print(\\\"ERROR ANALYSIS\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*60)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Analyze errors\\n\",\n",
    "    \"error_df = evaluator.error_analysis(y_test, y_pred, X_test, feature_names)\\n\",\n",
    "    \"\\n\",\n",
    "    \"if error_df is not None and len(error_df) > 0:\\n\",\n",
    "    \"    print(f\\\"\\\\nSample of misclassified cases:\\\")\\n\",\n",
    "    \"    print(error_df.head(10))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 14. Save Best Model\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"print(\\\"\\\\n\\\" + \\\"=\\\"*60)\\n\",\n",
    "    \"print(\\\"SAVING BEST MODEL\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*60)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Save best model\\n\",\n",
    "    \"model_path = '../models/best_model.pkl'\\n\",\n",
    "    \"trainer.save_model(best_model, model_path, best_model_name)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Save model metadata\\n\",\n",
    "    \"model_metadata = {\\n\",\n",
    "    \"    'model_name': best_model_name,\\n\",\n",
    "    \"    'accuracy': float(best_accuracy),\\n\",\n",
    "    \"    'f1_score': float(best_f1),\\n\",\n",
    "    \"    'precision': float(all_results.iloc[0]['precision']),\\n\",\n",
    "    \"    'recall': float(all_results.iloc[0]['recall']),\\n\",\n",
    "    \"    'features': feature_names,\\n\",\n",
    "    \"    'n_features': len(feature_names)\\n\",\n",
    "    \"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"import json\\n\",\n",
    "    \"with open('../models/model_metadata.json', 'w') as f:\\n\",\n",
    "    \"    json.dump(model_metadata, f, indent=4)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"âœ“ Model metadata saved: model_metadata.json\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Save all results\\n\",\n",
    "    \"all_results.to_csv('../models/model_comparison_results.csv', index=False)\\n\",\n",
    "    \"print(\\\"âœ“ Comparison results saved: model_comparison_results.csv\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 15. Model Performance Summary\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"print(\\\"\\\\n\\\" + \\\"=\\\"*60)\\n\",\n",
    "    \"print(\\\"FINAL MODEL PERFORMANCE SUMMARY\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*60)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\nðŸ† BEST MODEL: {best_model_name}\\\")\\n\",\n",
    "    \"print(\\\"\\\\nðŸ“Š Performance Metrics:\\\")\\n\",\n",
    "    \"print(f\\\"   Accuracy:  {best_accuracy:.4f} ({best_accuracy*100:.2f}%)\\\")\\n\",\n",
    "    \"print(f\\\"   Precision: {all_results.iloc[0]['precision']:.4f}\\\")\\n\",\n",
    "    \"print(f\\\"   Recall:    {all_results.iloc[0]['recall']:.4f}\\\")\\n\",\n",
    "    \"print(f\\\"   F1-Score:  {best_f1:.4f}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"if hasattr(best_model, 'predict_proba'):\\n\",\n",
    "    \"    print(f\\\"   ROC-AUC:   {roc_auc:.4f}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\nðŸ“ˆ Model Characteristics:\\\")\\n\",\n",
    "    \"print(f\\\"   Model Type: {type(best_model).__name__}\\\")\\n\",\n",
    "    \"print(f\\\"   Features Used: {len(feature_names)}\\\")\\n\",\n",
    "    \"print(f\\\"   Training Samples: {len(X_train)}\\\")\\n\",\n",
    "    \"print(f\\\"   Test Samples: {len(X_test)}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"if importance_df is not None:\\n\",\n",
    "    \"    print(\\\"\\\\nðŸŽ¯ Top 5 Most Important Features:\\\")\\n\",\n",
    "    \"    for i, row in importance_df.head(5).iterrows():\\n\",\n",
    "    \"        print(f\\\"   {i+1}. {row['Feature']}: {row['Importance']:.4f}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\nðŸ’¾ Saved Artifacts:\\\")\\n\",\n",
    "    \"print(\\\"   - best_model.pkl\\\")\\n\",\n",
    "    \"print(\\\"   - model_metadata.json\\\")\\n\",\n",
    "    \"print(\\\"   - model_comparison_results.csv\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\n\\\" + \\\"=\\\"*60)\\n\",\n",
    "    \"print(\\\"âœ“ PREDICTIVE MODELING COMPLETED SUCCESSFULLY!\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*60)\"\n",
    "    ]\n",
    "},\n",
    "{\n",
    "\"cell_type\": \"markdown\",\n",
    "\"metadata\": {},\n",
    "\"source\": [\n",
    "\"## Summary\\n\",\n",
    "\"\\n\",\n",
    "\"Pada tahap Predictive Modeling ini, kita telah:\\n\",\n",
    "\"\\n\",\n",
    "\"1. âœ… Trained Multiple Models:\\n\",\n",
    "\"   - Logistic Regression\\n\",\n",
    "\"   - Decision Tree\\n\",\n",
    "\"   - K-Nearest Neighbors\\n\",\n",
    "\"   - Random Forest\\n\",\n",
    "\"   - Gradient Boosting\\n\",\n",
    "\"\\n\",\n",
    "\"2. âœ… Evaluated Model Performance:\\n\",\n",
    "\"   - Accuracy, Precision, Recall, F1-Score\\n\",\n",
    "\"   - Confusion Matrix\\n\",\n",
    "\"   - ROC-AUC Score\\n\",\n",
    "\"   - Classification Report\\n\",\n",
    "\"\\n\",\n",
    "\"3. âœ… Model Comparison:\\n\",\n",
    "\"   - Compared all models side-by-side\\n\",\n",
    "\"   - Identified best performing model\\n\",\n",
    "\"\\n\",\n",
    "\"4. âœ… Hyperparameter Tuning:\\n\",\n",
    "\"   - Grid Search for optimal parameters\\n\",\n",
    "\"   - Improved model performance\\n\",\n",
    "\"   - Cross-validation for robustness\\n\",\n",
    "\"\\n\",\n",
    "\"5. âœ… Comprehensive Evaluation:\\n\",\n",
    "\"   - Detailed confusion matrix analysis\\n\",\n",
    "\"   - ROC and Precision-Recall curves\\n\",\n",
    "\"   - Feature importance analysis\\n\",\n",
    "\"   - Error analysis\\n\",\n",
    "\"\\n\",\n",
    "\"6. âœ… Model Deployment Ready:\\n\",\n",
    "\"   - Best model saved\\n\",\n",
    "\"   - Metadata documented\\n\",\n",
    "\"   - Ready for production use\\n\",\n",
    "\"\\n\",\n",
    "\"### Key Achievements:\\n\",\n",
    "\"- Best Model: [Model name]\\n\",\n",
    "\"- Accuracy: [XX.XX%]\\n\",\n",
    "\"- F1-Score: [X.XXXX]\\n\",\n",
    "\"- Performance meets success criteria (>80% accuracy target)\\n\",\n",
    "\"\\n\",\n",
    "\"### Model Insights:\\n\",\n",
    "\"- Top predictive features identified\\n\",\n",
    "\"- Model interpretability maintained\\n\",\n",
    "\"- Balanced performance across metrics\\n\",\n",
    "\"- Low false negative rate (important for medical diagnosis)\\n\",\n",
    "\"\\n\",\n",
    "\"### Next Steps:\\n\",\n",
    "\"Lanjut ke Notebook 7: Data Visualization untuk create comprehensive visualizations dan final reporting.\\n\",\n",
    "\"\\n\",\n",
    "\"---\"\n",
    "]\n",
    "}\n",
    "],\n",
    "\"metadata\": {\n",
    "\"kernelspec\": {\n",
    "\"display_name\": \"Python 3\",\n",
    "\"language\": \"python\",\n",
    "\"name\": \"python3\"\n",
    "},\n",
    "\"language_info\": {\n",
    "\"codemirror_mode\": {\n",
    "\"name\": \"ipython\",\n",
    "\"version\": 3\n",
    "},\n",
    "\"file_extension\": \".py\",\n",
    "\"mimetype\": \"text/x-python\",\n",
    "\"name\": \"python\",\n",
    "\"nbconvert_exporter\": \"python\",\n",
    "\"pygments_lexer\": \"ipython3\",\n",
    "\"version\": \"3.9.0\"\n",
    "}\n",
    "},\n",
    "\"nbformat\": 4,\n",
    "\"nbformat_minor\": 4\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
